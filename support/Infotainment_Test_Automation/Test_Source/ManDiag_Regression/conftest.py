"""
conftest.py ‚Äì Pytest Configuration (Generic Framework Template)
---------------------------------------------------------------
Framework responsibilities:
‚úî Generic Pytest hooks, reporting, summary
‚úî Console colored logs
‚úî HTML report customization

Project-specific responsibilities:
‚úî Imported from project_setup.py
"""

# ================================================================
#  Imports
# ================================================================
import logging
import pytest
import time

from colorama import Fore, Style
from pytest_metadata.plugin import metadata_key
from infotest.core.trace_logger import console_log

# ================================================================
#  Generic Framework-Wide Hooks (test tracking, summary, report)
# ================================================================
_test_start_times = {}
_test_summary = {"total": 0, "passed": 0, "failed": 0, "skipped": 0}
_session_start_time = 0.00


@pytest.hookimpl
def pytest_sessionstart(session):
    global _session_start_time
    _session_start_time = time.time()

@pytest.hookimpl(tryfirst=True)
def pytest_sessionfinish(session, exitstatus):
    session.config.stash[metadata_key].update({**version_info})

    global _session_start_time
    total_time = time.time() - _session_start_time
    if total_time is not None:
        print(f"\n=== Test session finished in {total_time:.2f} seconds ===\n")
    else:
        print("\n‚ö†Ô∏è Warning: Session start time not recorded (pytest_sessionstart may not have executed)\n")
    banner = (
        f"TOTAL: {_test_summary['total']} | "
        f"PASSED: {_test_summary['passed']} | "
        f"SKIPPED: {_test_summary['skipped']} | "
        f"FAILED: {_test_summary['failed']} | "
        f"‚è± {total_time:.2f}s"
    )
    session.config.stash[metadata_key]["Test Summary"] = banner
    border = "‚ïê" * (len(banner) + 4)
    console_log(logging.INFO, "\n")
    console_log(logging.INFO, Fore.CYAN + border)
    console_log(logging.INFO, Fore.CYAN + " " + banner + " ")
    console_log(logging.INFO, border + Style.RESET_ALL + "\n")


def pytest_runtest_logstart(nodeid, location):
    _test_start_times[nodeid] = time.time()
    _test_summary["total"] += 1
    console_log(logging.DEBUG, Fore.CYAN + f"üöÄ STARTING TEST: {nodeid}" + Style.RESET_ALL)


def pytest_runtest_logfinish(nodeid, location):
    start_time = _test_start_times.pop(nodeid, None)
    duration = f"‚è± {time.time() - start_time:.2f}s" if start_time else ""
    console_log(logging.DEBUG, Fore.MAGENTA + f"‚úÖ FINISHED TEST: {nodeid} {duration}\n" + Style.RESET_ALL)


def pytest_runtest_logreport(report):
    if report.when == "call":
        duration = f"‚è± {report.duration:.2f}s"
        if report.failed:
            _test_summary["failed"] += 1
            console_log(logging.ERROR, Fore.RED + f"\r‚ùå TEST FAILED: {report.nodeid} {duration}" + Style.RESET_ALL)
        elif report.skipped:
            _test_summary["skipped"] += 1
            console_log(logging.WARNING, Fore.YELLOW + f"\r‚ö†Ô∏è TEST SKIPPED: {report.nodeid} {duration}" + Style.RESET_ALL)
        else:
            _test_summary["passed"] += 1
            console_log(logging.INFO, Fore.GREEN + f"\r‚úÖ TEST PASSED: {report.nodeid} {duration}" + Style.RESET_ALL)


def pytest_html_report_title(report):
    report.title = "Infotainment Diagnostics Automation Report"


def pytest_html_results_summary(prefix, summary, postfix):
    prefix.extend([f"Report generated by Infotainment Automation Framework"])


def pytest_html_results_table_header(cells):
    cells.pop(-1)
    result_cell = cells.pop(0)
    cells.append(result_cell)
    cells.insert(1, "<th>Component</th>")
    cells.insert(2, "<th>Test Description</th>")


def pytest_html_results_table_row(report, cells):
    cells.pop(-1)
    result_cell = cells.pop(0)
    cells.append(result_cell)
    cells.insert(1, f"<td>{getattr(report, 'component', 'N/A')}</td>")
    cells.insert(2, f"<td>{getattr(report, 'description', 'N/A')}</td>")


@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    report = outcome.get_result()
    docstring = item.function.__doc__
    if docstring:
        report.component = docstring.split(":")[0].strip() if ":" in docstring else "N/A"
        report.description = docstring.split(":")[1].strip() if ":" in docstring else "N/A"
    else:
        report.component, report.description = "N/A", "N/A"


def pytest_configure(config):
    config.stash[metadata_key] = {}


def pytest_unconfigure(config):
    pass


# ================================================================
#  Import Project-Specific Setup (Simple Direct Import)
# ================================================================
import os
from configparser import ConfigParser

cfg = ConfigParser()
cfg.read("config.ini")
project_name = cfg.get("Project", "name", fallback="MIB4").strip().upper()

# Explicit imports for known projects
if project_name == "MIB4":
    from Project_Config.MIB4_project_setup import *
    print("[INFO] Loaded project setup: MIB4")

elif project_name == "HPCC":
    from Project_Config.HPCC_project_setup import *
    print("[INFO] Loaded project setup: HPCC")

elif project_name == "IHP":
    from Project_Config.IHP_project_setup import *
    print("[INFO] Loaded project setup: IHP")

elif project_name == "SEM25":
    from Project_Config.SEM25_project_setup import *
    print("[INFO] Loaded project setup: SEM25")

elif project_name == "EXP":
    # No hardware, just skip setup
    print("[INFO] Experimental mode: skipping hardware project setup")

else:
    raise ImportError(
        f"[ERROR] Unsupported project '{project_name}'. "
        f"Expected one of: MIB4, HPCC, EXP, EXP_TEST"
    )
